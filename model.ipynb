{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48463, 113, 200039)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "import numpy as np\n",
    "import glob \n",
    "\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "ONLY_ORGANIC = True\n",
    "TRAINING = True\n",
    "\n",
    "graph = Graph()\n",
    "for filename in glob.glob('only_organic_reduced_kgs/reduced_*' if ONLY_ORGANIC else 'reduced_kgs/reduced_*'):\n",
    "    graph.load(filename,format=filename.split('.')[-1])\n",
    "    \n",
    "entities = sorted(list(set(graph.subjects()) | set(graph.objects())))\n",
    "relations = sorted(list(set(graph.predicates())))\n",
    "len(entities), len(relations), len(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "effect_data = pd.read_csv('only_organic_effect_data_extra.csv' if ONLY_ORGANIC else 'effect_data_extra.csv')\n",
    "ent = set(map(str,entities))\n",
    "effect_data = effect_data[effect_data['species'].isin(ent)]\n",
    "effect_data = effect_data[effect_data['chemical'].isin(ent)]\n",
    "effect_data = effect_data[effect_data['smiles_clusters']>=0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from time import sleep\n",
    "from rdflib.namespace import Namespace\n",
    "schema = Namespace('https://schema.org/')\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from pubchempy import Compound\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "query = \"\"\"select ?cas ?pc where {\n",
    "  ?c wdt:P231 ?tmp ;\n",
    "     wdt:P662 ?pc .\n",
    "  bind(replace(?tmp,'-','') as ?cas)\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "\n",
    "extra_graph = Graph()\n",
    "\n",
    "try:\n",
    "    extra_graph.load('only_organic_physical_properties.ttl' if ONLY_ORGANIC else 'physical_properties.ttl',format='ttl')\n",
    "\n",
    "except FileNotFoundError:\n",
    "\n",
    "    results = get_results(endpoint_url, query)\n",
    "    for result in tq.tqdm(results[\"results\"][\"bindings\"]):\n",
    "        chem_id = 'https://cfpub.epa.gov/ecotox/cas/'+result['cas']['value']\n",
    "        if not chem_id in ent:\n",
    "            continue\n",
    "        vioxx = Compound.from_cid(int(result['pc']['value']))\n",
    "        mm = vioxx.molecular_weight\n",
    "        logp = vioxx.xlogp\n",
    "\n",
    "        if mm:\n",
    "\n",
    "            if mm < 600:\n",
    "                extra_graph.add((URIRef(chem_id),URIRef('http://example.org/compoundIsHeavy'),schema['False']))\n",
    "            else:\n",
    "                extra_graph.add((URIRef(chem_id),URIRef('http://example.org/compoundIsHeavy'),schema['True']))\n",
    "\n",
    "        if logp:\n",
    "            if logp < 4:\n",
    "                extra_graph.add((URIRef(chem_id),URIRef('http://example.org/compoundLogPCategory'),URIRef('http://example.org/low')))\n",
    "            elif logp < 5.5:\n",
    "                extra_graph.add((URIRef(chem_id),URIRef('http://example.org/compoundLogPCategory'),URIRef('http://example.org/medium')))\n",
    "            else:\n",
    "                extra_graph.add((URIRef(chem_id),URIRef('http://example.org/compoundLogPCategory'),URIRef('http://example.org/high')))\n",
    "\n",
    "    extra_graph.serialize('only_organic_physical_properties.ttl' if ONLY_ORGANIC else 'physical_properties.ttl',format='ttl')\n",
    "\n",
    "graph += extra_graph\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48468, 115, 203338)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = sorted(list(set(graph.subjects()) | set(graph.objects())))\n",
    "relations = sorted(list(set(graph.predicates())))\n",
    "len(entities), len(relations), len(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_mappings = {e:i for i,e in enumerate(entities)}\n",
    "relation_mappings = {e:i for i,e in enumerate(relations)}\n",
    "triples = np.asarray(list(map(lambda x: (entity_mappings[x[0]],\n",
    "                                         relation_mappings[x[1]],\n",
    "                                         entity_mappings[x[2]]),graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_distance_loss(w,epsilon=1.0):\n",
    "        \n",
    "    r = tf.reduce_sum(w*w, 1)\n",
    "\n",
    "    r = tf.reshape(r, [-1, 1])\n",
    "    D = r - 2*tf.matmul(w, tf.transpose(w)) + tf.transpose(r)\n",
    "    D = D + tf.linalg.diag(epsilon * tf.ones(D.shape[0]))\n",
    "    return tf.reduce_sum(tf.where(D<epsilon,1.0,0.0))/tf.cast(w.shape[1],tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from embedding_model import DistMult, ComplEx, TransE, ConvE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(entities)\n",
    "\n",
    "def create_negative(positive,n=2):\n",
    "    negative = np.repeat(positive,n,axis=0)\n",
    "    negative[:,0] = np.random.randint(0,M,size=len(positive)*n)\n",
    "    negative[:,2] = np.random.randint(0,M,size=len(positive)*n)\n",
    "    return negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.42 ms ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit create_negative(triples,n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(t,batch_size=32,n=2):\n",
    "    \n",
    "    t = t[np.random.randint(0,len(t),size=len(t))]\n",
    "    \n",
    "    num_batches = len(t)//batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        \n",
    "        positive = t[i*batch_size:i*batch_size+batch_size]\n",
    "        negative = create_negative(positive,n)\n",
    "        X = np.concatenate([positive,negative],axis=0)\n",
    "        y = np.concatenate([np.ones(len(positive)),-1*np.ones(len(negative))],axis=0)\n",
    "        \n",
    "        yield (X,y),y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4932338a744334aa45c407bd843476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bacc36916ed4a4a9176e82f295cd1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "total_models = 10\n",
    "total_dim = 200\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for n in tq.tqdm(range(total_models)):\n",
    "\n",
    "        embedding_model = ComplEx(entities,relations,dim=total_dim//total_models)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        losses = []\n",
    "        patience=10\n",
    "\n",
    "        for i in tq.tqdm(range(100),leave=False):\n",
    "\n",
    "            negative = create_negative(triples,n=32)\n",
    "\n",
    "            X = np.concatenate([triples,negative],axis=0)\n",
    "            y = np.concatenate([np.ones(len(triples)),-1*np.ones(len(negative))],axis=0)\n",
    "\n",
    "            hist = embedding_model.fit((X,y),y,\n",
    "                             batch_size=8192,\n",
    "                             shuffle=True,        \n",
    "                             verbose=0)\n",
    "\n",
    "            l = hist.history['loss'][-1]\n",
    "            losses.append(l)\n",
    "            if l < best_loss:\n",
    "                best_loss = l\n",
    "                c = 0\n",
    "            else:\n",
    "                c += 1\n",
    "\n",
    "            if c > patience: break\n",
    "\n",
    "        embeddings.append(embedding_model.get_layer('entity_embedding').get_weights()[0])\n",
    "        \n",
    "    W = np.concatenate(embeddings,axis=1)\n",
    "    np.save('organic_only_W.npy' if ONLY_ORGANIC else 'W.npy',W)\n",
    "        \n",
    "else:\n",
    "    W = np.load('organic_only_W.npy' if ONLY_ORGANIC else 'W.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "if TRAINING:\n",
    "    plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = {}\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from pubchempy import Compound\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "query = \"\"\"select ?cas ?pc where {\n",
    "  ?c wdt:P231 ?tmp ;\n",
    "     wdt:P662 ?pc .\n",
    "  bind(replace(?tmp,'-','') as ?cas)\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "try: \n",
    "    fps = pd.read_pickle('fingerprints.pkl')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    \n",
    "    results = get_results(endpoint_url, query)\n",
    "    for result in tq.tqdm(results[\"results\"][\"bindings\"]):\n",
    "        chem_id = 'https://cfpub.epa.gov/ecotox/cas/'+result['cas']['value']\n",
    "        if chem_id in set(effect_data.chemical.values): \n",
    "            vioxx = Compound.from_cid(int(result['pc']['value']))\n",
    "            fps[chem_id] = vioxx.fingerprint\n",
    "    pd.to_pickle(fps,'fingerprints.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bin(he): \n",
    "    scale = 16 ## equals to hexadecimal\n",
    "    num_of_bits = 900\n",
    "    return bin(int(he, scale))[2:].zfill(num_of_bits)\n",
    "\n",
    "effect_data['fp'] = [to_bin(fps[c]) if c in fps else to_bin('0') for c in effect_data['chemical'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_data = effect_data[effect_data['fp']!=to_bin('0')]\n",
    "effect_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = effect_data[['fp','species','chemical','conc (mol/L)']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray([[fp,\n",
    "                    entity_mappings[URIRef(s)],\n",
    "                    entity_mappings[URIRef(c)],\n",
    "                    conc] for fp,s,c,conc in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(true,pred):\n",
    "    return tf.reduce_mean(tf.where(true-pred>1,abs(true-pred),(true-pred)**2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "epsilon = 1e-6\n",
    "\n",
    "@tf.function\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  tf.reduce_sum((y_true-y_pred)**2)\n",
    "    SS_tot = tf.reduce_sum((y_true-tf.reduce_mean(y_true))**2)\n",
    "    return ( 1 - SS_res/(SS_tot + epsilon))\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate, LayerNormalization, BatchNormalization, Activation, Input, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "def mlp(hidden=[128]):\n",
    "    \n",
    "    model = Sequential()\n",
    "    for h in hidden:\n",
    "        model.add(Dense(h))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(tf.keras.activations.swish))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='mae',metrics=[r2_keras])\n",
    "    return model\n",
    "   \n",
    "m = mlp()\n",
    "m.build(input_shape=(1,100))\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(dim):\n",
    "    encoder = tf.keras.models.Sequential([Dropout(0.2),Dense(200)])\n",
    "    model = tf.keras.models.Sequential([encoder,Dropout(0.2),Dense(dim,activation='sigmoid')])\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "    return model, encoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def kfold_group(X,y,groups,k=5):\n",
    "    \n",
    "    group_idx = [np.random.permutation(np.where(groups==g)[0]) for g in np.unique(groups)]\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    for j,g in enumerate(group_idx):\n",
    "        g_fold = []\n",
    "        for l in np.array_split(g,k):\n",
    "            g_fold.append(l)\n",
    "        folds.append(g_fold)\n",
    "        \n",
    "    for i in range(k):\n",
    "        test = np.concatenate([folds[j][i] for j in range(len(group_idx))])                          \n",
    "        train = np.concatenate([np.concatenate([folds[j][m] for j in range(len(group_idx))]) for m in range(k) if m!=i]).ravel()\n",
    "        \n",
    "        yield train,test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chemicals = set(data[:,1].astype(int))\n",
    "species = set(data[:,2].astype(int))\n",
    "chemical_mapping = {k:i for i,k in enumerate(chemicals)}\n",
    "species_mapping = {k:i for i,k in enumerate(species)}\n",
    "\n",
    "def data_generator(X,Y,use_embedding=False):\n",
    "\n",
    "    if use_embedding:\n",
    "        x = np.asarray(list(map(lambda x:np.concatenate([W[x[0]],W[x[1]]],axis=0),X)))\n",
    "    else:\n",
    "        v1 = np.zeros((len(X),len(chemicals)))\n",
    "        v2 = np.zeros((len(X),len(species)))\n",
    "        for j,(a,b) in enumerate(X):\n",
    "            v1[j,chemical_mapping[a]] = 1.0\n",
    "            v2[j,species_mapping[b]] = 1.0\n",
    "        x = np.concatenate([v1,v2],axis=1)\n",
    "\n",
    "    return x,Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold, GroupShuffleSplit, LeaveOneGroupOut, StratifiedKFold\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "\n",
    "y = data[:,3].astype('float32')\n",
    "X = data[:,1:3].astype(int)\n",
    "\n",
    "groups = effect_data['smiles_clusters'].values\n",
    "folds = 5\n",
    "EPOCHS = 1000\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "N = 10\n",
    "\n",
    "scores = []\n",
    "bs = len(y)\n",
    "\n",
    "oof = np.zeros(y.shape)\n",
    "oof_embedding = np.zeros(y.shape)\n",
    "\n",
    "for _ in tq.tqdm(range(N)):\n",
    "\n",
    "    oof_this_seed = np.zeros(y.shape)\n",
    "    oof_this_seed_embedding = np.zeros(y.shape)\n",
    "    \n",
    "    for i,(train,test) in tq.tqdm(enumerate(kfold_group(X,y,groups=groups,k=folds)),\n",
    "                                  total=folds,\n",
    "                                  desc='Folds',\n",
    "                                  leave=False):\n",
    "\n",
    "        \n",
    "        model = mlp(hidden=[128,64])\n",
    "        \n",
    "        model.fit(*data_generator(X[train],y[train]),\n",
    "                  validation_data=data_generator(X[test],y[test]),\n",
    "                  epochs=EPOCHS,verbose=0,\n",
    "                  batch_size=bs,shuffle=True,\n",
    "                 callbacks=[tf.keras.callbacks.EarlyStopping('val_r2_keras',mode='max',patience=10,restore_best_weights=True)])\n",
    "\n",
    "        p = model.predict(data_generator(X[test],y[test])[0],batch_size=bs).ravel()\n",
    "        oof[test] += p/N\n",
    "        oof_this_seed[test] += p\n",
    "\n",
    "        model = mlp(hidden=[128,64])\n",
    "        \n",
    "        model.fit(*data_generator(X[train],y[train],use_embedding=True),\n",
    "                  validation_data=data_generator(X[test],y[test],use_embedding=True),\n",
    "                  epochs=EPOCHS,verbose=0,\n",
    "                  batch_size=bs,shuffle=True,\n",
    "                 callbacks=[tf.keras.callbacks.EarlyStopping('val_r2_keras',mode='max',patience=10,restore_best_weights=True)])\n",
    "        \n",
    "        p = model.predict(data_generator(X[test],y[test],use_embedding=True)[0],batch_size=bs).ravel()\n",
    "        oof_embedding[test] += p/N\n",
    "        oof_this_seed_embedding[test] += p\n",
    "        \n",
    "    scores.append([r2_score(y,oof_this_seed),r2_score(y,oof_this_seed_embedding)])\n",
    "    \n",
    "    print('Out-of-fold R^2',scores[-1])\n",
    "\n",
    "s,s_embedding = np.split(np.asarray(scores),2,axis=1)\n",
    "\n",
    "'Out-of-fold R^2',np.mean(s),'+-',np.std(s),'Embedding:',np.mean(s_embedding),'+-',np.std(s_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=dict(species=effect_data.species,chemical=effect_data.chemical,prediction=oof)).to_csv('only_organic_predictions.csv' if ONLY_ORGANIC else 'predictions.csv')\n",
    "pd.DataFrame(data=dict(species=effect_data.species,chemical=effect_data.chemical,prediction=oof_embedding)).to_csv('only_organic_predictions_embedding.csv' if ONLY_ORGANIC else 'predictions_embedding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
